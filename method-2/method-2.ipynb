{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKSWN8LsGIY6"
      },
      "source": [
        "# Evaluation of Language Identification Models (2 Speechbrain models and Whisper by OpenAI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_bej1KwNPWD"
      },
      "source": [
        "## Requirements (installations, imports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez5umRZ7AacA"
      },
      "outputs": [],
      "source": [
        "# Mount drive\n",
        "# Für andere Authoren: Verknüpfung von geteiltem Ordner in eigener Ablage erstellen; eventuell Links anpassen\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26mtcgITv96N"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "install = True # Change if already installed\n",
        "if install:\n",
        "  ! pip install speechbrain\n",
        "  ! pip install git+https://github.com/openai/whisper.git\n",
        "  ! pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJNHAf4BvJ0E"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "\n",
        "import os\n",
        "\n",
        "import whisper\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from speechbrain.pretrained import EncoderClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umxqZr2vxESo"
      },
      "outputs": [],
      "source": [
        "# Check for existing audio backend\n",
        "print(str(torchaudio.get_audio_backend()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQxyy0ez3utT"
      },
      "source": [
        "## Speechbrain model; trained on VoxLingua107; 107 languages; Official error rate: 7%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXL8CockvQTP"
      },
      "outputs": [],
      "source": [
        "# Get language identifier from speechbrain\n",
        "language_id = EncoderClassifier.from_hparams(source=\"speechbrain/lang-id-voxlingua107-ecapa\", savedir=\"tmp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Prn6kkYEqkZ"
      },
      "source": [
        "## Whisper; trained on multiple sources; 99 languages; Unknown accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u851OmY_xgv9"
      },
      "outputs": [],
      "source": [
        "# Check for GPU and set device\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Set Runtime to GPU in Google Colab\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKtxlo_HLfMk"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Available Models:\n",
        "Size      Parameters\tEnglish-only  model\t    Multilingual model\t    Required VRAM\tRelative speed\n",
        "tiny\t    39 M\t      tiny.en\t      tiny\t    ~1 GB\t                  ~32x\n",
        "base\t    74 M\t      base.en\t      base\t    ~1 GB\t                  ~16x\n",
        "small\t    244 M\t      small.en\t    small\t    ~2 GB\t                  ~6x\n",
        "medium\t  769 M\t      medium.en\t    medium\t  ~5 GB\t                  ~2x\n",
        "large\t    1550 M\t    N/A\t          large\t    ~10 GB\t                1x\n",
        "\"\"\"\n",
        "\n",
        "# Choose and load model\n",
        "model = whisper.load_model(\"medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-IX4QtECWrJ"
      },
      "source": [
        "## Combination evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH7KH89rCaND"
      },
      "outputs": [],
      "source": [
        "# Set path to directory to be evaluated\n",
        "dir = '/content/drive/MyDrive/Language Identification/Fleurs_dev/'\n",
        "\n",
        "sample_no = 20\n",
        "\n",
        "\n",
        "group_1 = ['hr', 'da', 'nl', 'en', 'fi', 'fr', 'gl', 'de', 'it'] # WE\n",
        "group_2 = ['bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'ne', 'pa', 'ur'] # SA\n",
        "\n",
        "groups = [group_1, group_2]\n",
        "\n",
        "# Use model to identify language for every file in the specified directory\n",
        "g = 1\n",
        "\n",
        "all = 0\n",
        "all_correct_tdnn = 0\n",
        "all_correct_whisper = 0 \n",
        "\n",
        "for group in groups:\n",
        "  whisper_acc = {}\n",
        "  tdnn_acc = {}\n",
        "\n",
        "  for ln in group:\n",
        "\n",
        "    if ln == 'zh':\n",
        "      continue\n",
        "\n",
        "    path = dir + ln + '/'\n",
        "\n",
        "    total = 0\n",
        "    whisper_correct = 0\n",
        "    tdnn_correct = 0\n",
        "\n",
        "    for f in os.listdir(path)[0:sample_no]:\n",
        "      file_path = path + f\n",
        "\n",
        "      # load audio and pad/trim it to fit 30 seconds\n",
        "      audio = whisper.load_audio(file_path)\n",
        "      audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "      # make log-Mel spectrogram and move to the same device as the model\n",
        "      mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "      # detect the spoken language\n",
        "      _, probs = model.detect_language(mel)\n",
        "      ln_whisper = max(probs, key=probs.get)\n",
        "\n",
        "      if ln_whisper=='jw':\n",
        "        ln_whisper = 'jv'\n",
        "      elif ln_whisper=='no':\n",
        "        ln_whisper='nb'\n",
        "\n",
        "      if (ln_whisper == ln) or (ln_whisper=='zh' and (ln=='yue' or ln=='cmn')):\n",
        "        whisper_correct += 1\n",
        "        all_correct_whisper += 1\n",
        "\n",
        "      if ln_whisper in group:\n",
        "        signal = language_id.load_audio(file_path)\n",
        "        prediction =  language_id.classify_batch(signal)\n",
        "\n",
        "        # Get values from output neurons belonging to specific languages, save them to a list\n",
        "\n",
        "        group_1_dict = {'hr':prediction[0][0][36], 'da':prediction[0][0][17], 'nl':prediction[0][0][68], 'en':prediction[0][0][20], \n",
        "                        'fi':prediction[0][0][26], 'fr':prediction[0][0][28], 'gl':prediction[0][0][29], 'de':prediction[0][0][18],\n",
        "                        'it':prediction[0][0][43]}\n",
        "\n",
        "        group_2_dict = {'bn':prediction[0][0][9], 'gu':prediction[0][0][31], 'hi':prediction[0][0][35], 'kn':prediction[0][0][50],\n",
        "                        'ml':prediction[0][0][61], 'mr':prediction[0][0][63], 'ne':prediction[0][0][67], 'pa':prediction[0][0][72], \n",
        "                        'ur':prediction[0][0][100]}\n",
        "\n",
        "        # Get key of maximum value\n",
        "        if group == group_1:\n",
        "          ln_tdnn = max(group_1_dict, key=group_1_dict.get)\n",
        "        elif group == group_2:\n",
        "          ln_tdnn = max(group_2_dict, key=group_2_dict.get)\n",
        "\n",
        "        if (ln_tdnn == ln) or (ln_tdnn=='cmn' and ln=='yue'):\n",
        "          tdnn_correct += 1\n",
        "          all_correct_tdnn += 1\n",
        "\n",
        "      total += 1\n",
        "      all += 1\n",
        "\n",
        "    whisper_acc[ln] = whisper_correct / total\n",
        "    tdnn_acc[ln] = tdnn_correct / total\n",
        "  \n",
        "  sum = 0\n",
        "  for val in whisper_acc.values():\n",
        "    sum += val\n",
        "  whisper_avg = sum / len(whisper_acc)\n",
        "\n",
        "  sum = 0\n",
        "  for val in tdnn_acc.values():\n",
        "    sum += val\n",
        "  tdnn_avg = sum / len(tdnn_acc)\n",
        "\n",
        "  if whisper_avg>=tdnn_avg:\n",
        "    recommendation = 'No combination recommended for Group ' + str(g) + '.'\n",
        "  else:\n",
        "    recommendation = 'Combination recommended for Group ' + str(g) + '.'\n",
        "\n",
        "  print('Group', str(g))\n",
        "  print('Whisper: ', whisper_acc, '; Avg: ', whisper_avg)\n",
        "  print('Combination: ', tdnn_acc, '; Avg: ', tdnn_avg)\n",
        "  print(recommendation, '\\n')\n",
        "\n",
        "  g += 1\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8_bej1KwNPWD",
        "oQxyy0ez3utT",
        "-Prn6kkYEqkZ",
        "z-IX4QtECWrJ"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
